<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="description" name="description">
  <meta name="google" content="notranslate" />
 

  <!-- Disable tap highlight on IE -->
  <meta name="msapplication-tap-highlight" content="no">
  
  <link rel = "icon" href = "./assets/images/website-logo1.jpg" type = "image/x-icon">

  <title>Projects-Summary</title>  

<link href="./main.3f6952e4.css" rel="stylesheet"></head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
  <nav class="navbar  navbar-fixed-top navbar-default">
    <div class="container">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      <div class="collapse navbar-collapse" id="navbar-collapse">
        <ul class="nav navbar-nav ">
          <li><a href="./index.html" title="">01 : Home</a></li>
          <li><a href="./projects.html" title="">02 : Projects</a></li>
          <li><a href="./about.html" title="">03 : About me</a></li>
          <li><a href="./contact.html" title="">04 : Contact</a></li>
          <li><a href="./experience.html" title="">05 : Experience</a></li>
          <li><a href="./achievements.html" title="">06 : Achievements</a></li>
          <li><a href="./assets/Tarun-Data Science Resume.pdf" title="" target="_blank">07 : Resume</a></li>
        </ul>


          <ul class="nav navbar-nav navbar-right navbar-small visible-md visible-lg">
            <li><a href="./projects.html" title="" class="active">BACK</a></li>
          </ul>


      </div> 
    </div>
  </nav>
</header>
<div class="section-container">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <img src="./assets/images/fr.jpeg" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">001 : Fake News Detection using a Chatbot</h1>
          </div>

            <li>1.This is my Data Mining final project. Through proper data cleaning and Exploratory Data Analysis(EDA), we gained insights into the dataset, allowing us to understand the characteristics and patterns of the data.</li><br>
            <li>2.Feature engineering using TF-IDF vectorization technique transformed the text data into numerical features suitable for training machine learning models.</li><br>
            <li>3.We trained various models including Logistic Regression, Decision Tree, Random Forest, Naive Bayes, K-Nearest Neighbors, Support Vector Machines, Passive Aggressive Classifiers, Multi-Layer Perceptron, Convolutional Neural Network (CNN), Recurrent Neural Network(RNN), Long short-term memory(LSTM), Gated recurrent unit(GRU).</li><br>
            <li>4.The Passive Aggressive Classifier model emerged as the best performing model, achieving high accuracy, precision, recall, and ROC AUC scores in distinguishing between real and fake news articles.</li><br>
            <li>5.Hyperparameter tuning further optimized the Passive Aggressive Classifier model, enabling us to identify the best combination of hyperparameters to improve its performance.</li><br>
            <li>6.We updated the Passive Aggressive Classifier model using the optimal hyperparameters and saved it for future use, ensuring consistency and reproducibility.</li><br>
            <li>7.Cross-validation was performed to assess the model's generalization ability and validate its performance on unseen data, further confirming its effectiveness.</li><br>
            <li>8.We then used Bidirectional Encoder Representations from Transformers - BERT base model(uncased); Number of Layers of transformer encoders: 12; Hidden Size: output of each transformer layer is a vector of size 768; Attention Heads: hyperparameter that determines how many parallel attention operations are performed(12); Total Parameters(including weights and biases): 110 million; Vocabulary Size: 30,000. </li><br>
            <li>9.The BERT model outperformed Passive Aggressive Classifier model. So we decided to test both the models to check there performances.</li><br>
            <li>10.Testing the BERT model & Passive Aggressive Classifier model on few articles in the dataset, reinforcing its reliability for predicting news authenticity.</li><br>
            <li>11.Based on the satisfactory results, we built a chatbot that utilizes the models to predict the authenticity of news articles, providing users with informed insights. </li><br>
            <li>11.The integration and deployment of the chatbot offer a practical tool to combat fake news, sentiment of the news. Which predicts the news and the model gets updated based on the feedback.</li>






          <blockquote>
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>


      <div class="col-xs-12">
        <img src="./assets/images/rs.jpeg" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">002 : Recommender System</h1>
          </div>

            <li>In the IBM Machine Learning Capstone project, I was able to implement a recommender system project. The main goal of this project is to improve learnersâ€™ learning experience via helping them quickly find new interested courses and better paving their learning paths.</li><br>
            <li>The tasks I performed:</li><br>
            <li>1.Collecting and understanding data</li><br>
            <li>2.Performing exploratory data analysis on online course enrollments datasets</li><br>
            <li>3.Extracting Bag of Words (BoW) features from course textual content</li><br>
            <li>4.Calculating course similarity using BoW features</li><br>
            <li>5.Building content-based recommender systems using various unsupervised learning algorithms, such as: Distance/Similarity measurements, K-means, Principal Component Analysis (PCA), etc.</li><br>
            <li>6.Building collaborative-filtering recommender systems using various supervised learning algorithms: K Nearest Neighbors, Non-negative Matrix Factorization (NMF), Neural Networks, Linear Regression, Logistic Regression, RandomForest, etc.</li><br>
            <li>7.Creating an insightful and informative slideshow and presenting it.</li><br>
            <li>8.Tools: Jupyter Notebooks and Watson Studio; Libraries: Pandas, NumPy, Matplotlib, Seaborn, ipython-sql, Scikit-learn, ScipPy, Keras, and TensorFlow.</li>

          <blockquote>
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli/IBM-Machine-Learning-Professional-Certificate-Capstone-Project" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>


      <div class="col-xs-12">
        <img src="./assets/images/cyc.png" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">003 : Cyclist Scenario & Google Fiber Scenario</h1>
          </div>

            <li>1.Cyclist Scenario & Google Fiber Scenario was my capstone project for the Google Business Intelligence course. During the project,</li><br>
            <li>2.Learned how an actual project is done at the workplace, approaching a project using structured thinking by following specific steps in a specific order. </li><br>
            <li>3.Meeting notes was provided, understood the project background, stakeholders, team members, Project approvals and dependencies, project goals, the deliverables and metrics, measure success, asking right questions and other considerations.</li><br>
            <li>4.Organising tasks into milestones, main importantly created documents Stakeholder requirement document, Project requirement document, strategy document.</li><br>
            <li>5.Learned Google DataFlow but I have used BigQuery console created target tables for the final dashboard from the datasets provided.</li><br>
            <li>6.Used the target table created to design a BI viz that will address questions, started by creating a mockup, then made my charts, a dashboard, and a brief presentation that summarises the work I did on this project.</li><br>
            <li>7.Skills, Tools & Technologies: Data Modeling, Business Analysis, Bigquery, Sheets, Data Analysis, Data Pipelines ETL/ELT, Business Process, Tableau Software, Data Visualization(DataViz), Business Intelligence, SQL, Dashboarding and Reporting, KPI, Design Patterns, Database Schemas, Data storage systems, database and Pipeline optimisation.</li>

          <blockquote>
            
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli/Google-Business-Intelligence-Project-Google-Fiber" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>


      <div class="col-xs-12">
        <img src="./assets/images/ts.png" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">004 : Forecasting Weight Dynamics and Visualizing Historical Data - A Time Series Analysis</h1>
          </div>

            <li>1.Objective:</li><br>
            <li>1.1.Developing forecasting models to predict an insect colony weight using colony and environmental data.</li><br>
            <li>1.2.Created a Power BI dashboard to visualize the relationships between colony weight and other factors, as well as the historical data overall.</li><br>
            <li>2.Features: Date, Colony Activity, Dead Colony Weight, Nest Temperature, Nest Humidity, Red Luminous Intensity, Green Luminous Intensity, Blue Luminous Intensity, White Luminous Intensity, IR Luminous Intensity, Sound Intensity, Nest Weight</li><br>
            <li>3.Performed: Exploratory Data Analysis: Exploring both colony data and environmental data to understand their characteristics and identify patterns and correlations and handle outliers.</li><br>
            <li>4.Feature Engineering: Engineered additional features that capture the relationships between colony features and environmental factors.</li><br>
            <li>5.Model Development:</li><br>
            <li>5.1.Developed time series forecasting models for the nest weight using techniques like ARIMA, SARIMA, linear regression, decision trees, generalized additive models, or machine learning algorithms.</li><br>
            <li>5.2.Highlighted the interpretability of the models, such as examining coefficients, feature importance, or partial dependence plots.</li><br>
            <li>6.Model Evaluation: Based on evaluation metrics (MSE, RMSE, MAPE)</li><br>
            <li>6.1.Assessed model performance using MSE over a test set that will be provided to you along with the training dataset.</li><br>
            <li>6.2.Explained the interpretability and insights derived from the models.</li><br>
            <li>7.Power BI Dashboard: Created an interactive dashboard to visualize historical data effectively.</li>

          <blockquote>
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli/Forecasting-Weight-Dynamics-and-Visualizing-Historical-Data-A-Time-Series-Analysis" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>


      <div class="col-xs-12">
        <img src="./assets/images/s&S.png" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">005 : Sauce & Spoon</h1>
          </div>

            <li>1.Sauce & Spoon was my capstone project for the Google Project Management course. During the project,</li><br>
            <li>2.Analyzed project documents and supporting materials to identify project requirements, evaluate stakeholders and problem-solve.</li><br>
            <li>3.Completed a project charter and used it as a tool to align project scope and goals among stakeholders.</li><br>
            <li>4.Added specificity to project goals to make them SMART and applied practical negotiation skills with stakeholders to prioritize project goals.</li><br>
            <li>5.Built a project plan by examining project documentation, conducting online research, analyzing critical conversations to identify tasks and milestones, and then documenting and prioritizing them in a project plan.</li><br>
            <li>6.Demonstrated effective communication techniques for making accurate time estimates for project tasks.</li><br>
            <li>7.Defined and described quality management standards and evaluated against those standards to ensure that the project achieved the required quality.</li><br>
            <li>8.Generated evaluation questions from survey questions and recognized how to share qualitative data effectively.</li><br>
            <li>9.Learned strategies to facilitate a productive retrospective by encouraging participation, accountability, and positivity.</li><br>
            <li>10.Learned how to communicate and escalate project problems to stakeholders and demonstrated my impact through effective reporting strategies.</li>
            

          <blockquote>
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli/Google-Project-Management-Capstone-Project" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>


      <div class="col-xs-12">
        <img src="./assets/images/006.png" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">006 : Analyzing FitBit Fitness Tracker App data for Bellabeat</h1>
          </div>

            <li>1.Project Summary: The Google CAPSTONE project involved leveraging data analytics skills to analyze FitBit Fitness Tracker App data for Bellabeat, a leading health-focused smart product manufacturer for women. The primary goal was to extract valuable insights that could contribute to the growth and marketing strategy of Bellabeat.</li><br>
            <li>2.Objectives Achieved: </li><br>
            <li>2.1 Identifying Trends - The project successfully identified key trends within the FitBit Fitness Tracker App data. This involved in-depth analysis and visualization using Python, uncovering patterns that provided actionable insights.</li><br>
            <li>2.2 Applicability to Bellabeat Customers: By understanding the FitBit app usage patterns, the project team extrapolated insights applicable to Bellabeat's customer base. This step involved connecting the FitBit data trends to potential opportunities and challenges within the health and wellness tech market.</li><br>
            <li>2.3 Influence on Marketing Strategy: The insights gained were strategically analyzed to propose recommendations for Bellabeat's marketing team. This step aimed to align the company's approach with consumer behavior, ensuring a targeted and effective marketing strategy.</li><br>
            <li>3.Key Contributions to Employability:</li><br>
            <li>3.1 Portfolio Enhancement: The project serves as a valuable addition to the portfolio, showcasing practical experience in data analytics, Python programming, and strategic thinking in a real-world business context.</li><br>
            <li>3.2.Demonstrating Technical Expertise: Application of Python for data cleaning, transformation, visualization, and analysis demonstrates technical proficiency in handling diverse aspects of the data analytics process.</li><br>
            <li>3.3.Communication Skills: The ability to communicate findings effectively, linking data insights to business objectives, is highlighted through the project's summary and presentation.</li><br>
            <li>3.4.Value Proposition to Employers: The project emphasizes the candidate's ability to translate raw data into meaningful business implications, providing a unique value proposition to potential employers.</li><br>
            <li>4.Potential Interview Talking Points: </li><br>
            <li>4.1: Analytical Approach: Discuss the methodologies and techniques employed to analyze the FitBit data, showcasing problem-solving skills.</li><br>
            <li>4.2.Business Impact: Highlight the potential impact of the insights on Bellabeat's marketing strategy and overall business growth.</li><br>
            <li>4.3.Challenges Overcome: Describe any challenges faced during the project and the strategies implemented to address them</li><br>
            <li>4.4.Future Recommendations: Discuss potential future steps or recommendations based on the analysis, showcasing forward-thinking and strategic planning.</li><br>
            <li>5.In summary, the CAPSTONE project effectively blends technical expertise with a strategic mindset, demonstrating the ability to generate actionable insights from real-world data for the benefit of a high-tech wellness company like Bellabeat.</li>
            

          <blockquote>
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli/google-data-analytics-capstone-project" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>


      <div class="col-xs-12">
        <img src="./assets/images/007.jpeg" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">007 : Statistical Data Analysis on Human Activity Recognition Data Set</h1>
          </div>

            <li>1.Project Summary: The project focused on conducting statistical data analysis on the Human Activity Recognition Dataset, aiming to contribute to the development of a Healthcare Monitoring app. The goal was to accurately predict physical activity using sensor data (accelerometer, gyroscope) from mobile phones, with potential applications in healthcare, sports, and fitness.</li><br>
            <li>2.Understanding the Data: Thoroughly examined the Human Activity Recognition Dataset, identifying features crucial for predicting physical activity. Established a clear problem statement to guide the project, emphasizing the development of a healthcare monitoring app leveraging sensor data.</li><br>
            <li>3.Data Pre-Processing and Cleaning: Conducted comprehensive data pre-processing, addressing missing values and outliers to ensure data quality. Utilized descriptive statistics for gaining insights into variable characteristics and relationships, employing graphical methods for visualization.</li><br>
            <li>4.Multidimensional Data Analysis: Applied hierarchical and K-means clustering techniques to identify distinct groups within the dataset. Conducted correlation analysis to handle high-dimensional data, utilizing dimensionality reduction techniques for enhanced model performance.</li><br>
            <li>5.Feature Selection: Employed Lasso Regression for feature selection, ensuring the model's focus on relevant variables for accurate predictions.</li><br>
            <li>6.Classification Models: Utilized Support Vector Machine, Logistic Regression, Decision Tree, and Random Forest for classification tasks. Constructed confusion matrices and evaluated models using accuracy, precision, Kappa statistics, recall, and F-1 score to determine the most suitable model.</li><br>
            <li>7.Validation and Model Assessment: Employed K-fold cross-validation to ensure the generalizability of the selected model and prevent overfitting. Demonstrated a rigorous evaluation process to validate the robustness and reliability of the chosen classification model.</li><br>
            <li>8.Project Impact and Future Work: The project's outcomes have the potential to significantly impact healthcare monitoring applications. Future work may involve refining the model, incorporating additional features, and exploring real-world implementation scenarios.</li>

          <blockquote>
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli/Statistical-Data-Analysis-on-Human-Activity-Recognition-dataset-UCI-Repository-" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>


      <div class="col-xs-12">
        <img src="./assets/images/008.png" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">008 : Predicting Housing Price of California Census Data using Machine Learning</h1>
          </div>

            <li>1.Our task is to use the California census data to build a model to find the housing prices in the state. This data includes features such as Population, Median Income and Median housing price for each block group in California.</li><br>
            <li>2.A block group has a population between 600 to 3,000. We will call them "districts" for short. Our model should be able to predict the median housing price for any district.</li><br>
            <li>3.We have performed Exploration of California census data, Data Pre-Processing, Data Preparation, Model Development and Training, Model Evaluation and Validation, Fine Tuning using Hyper Parameters</li><br>
            <li>4.We have measured the performance of different models using the Root Mean Squared Error (RMSE). Also predicted the median_housing_price.</li><br>
            <li>5.Random forest model was best suited for our data set. Hence used it for the fine tuning using grid search.</li>

          <blockquote>
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli/Prediction-of-Housing-Price-of-California-Census-Data-using-Machine-Learning" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>


      <div class="col-xs-12">
        <img src="./assets/images/009.png" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">009 : Missing value imputation for titanic dataset using mean imputation technique in OpenMP</h1>
          </div>

            <li>It is a Final Group Project, for Data Management & Comuputer Networks. We have used The UNINA cluster(University): 32 nodes, 128 GPUs. Accessing the cluster: the user interface.</li><br>
            <li>Programming with MPI and Open MPI. Missing value imputation for titanic dataset using mean imputation technique in Openmp and plotted a graph for execution time Vs number of threads.</li><br>
            <li>We have submitted our work to LECTURER: Prof. Guido Russo which got a great Result.</li>

          <blockquote>
            <p><a href="https://github.com/VenkataTarunKumarMavillapalli/Missing-Value-Imputation-for-Titanic-Dataset-using-mean-imputation-technique-in-Openmp" title="" class="fa-icon">
              <i class="fa fa-github" style="font-size:30px"></i>
            </a></p>
          </blockquote>
        </div>
      </div>







    </div>
  </div>
</div>




<script>
  document.addEventListener("DOMContentLoaded", function (event) {
     navActivePage();
  });
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID 

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-XXXXX-X', 'auto');
  ga('send', 'pageview');
</script>

--> <script type="text/javascript" src="./main.70a66962.js"></script></body>

</html>
